import { createClient } from '@supabase/supabase-js';
import OpenAI from 'openai';
import { NextResponse } from 'next/server';
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
import mammoth from 'mammoth';


const url = process.env.NEXT_PUBLIC_SUPABASE_URL!;
const anonKey = process.env.NEXT_PUBLIC_SUPABASE_PUBLISHABLE_DEFAULT_KEY!;
const serviceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
const supabaseStorage = createClient(url, serviceKey || anonKey);
const supabase = createClient(url, anonKey);
const openai = new OpenAI();


function safeDecodeURIComponent(str: string): string {
  try { 
    return decodeURIComponent(str); 
  } catch { 
    try { 
      return decodeURIComponent(str.replace(/%/g, '%25')); 
    } catch { 
      return str; 
    } 
  }
}

async function extractTextFromFile(file: File): Promise<string> {
    const buffer = Buffer.from(await file.arrayBuffer());
    const fileName = file.name.toLowerCase();

    if (fileName.endsWith('.pdf')) {
        const PDFParser = (await import('pdf2json')).default;
        
        return new Promise((resolve, reject) => {
            const pdfParser = new (PDFParser as any)(null, true);

            pdfParser.on('pdfParser_dataError', (err: any) => 
                reject(new Error(`Error parsing PDF: ${err.parserError}`))
            );

            pdfParser.on('pdfParser_dataReady', (pdfData: any) => {
                try {
                    let fullText = '';
                    pdfData.Pages?.forEach((page: any) => 
                        page.Texts?.forEach((text: any) => 
                            text.R?.forEach((r: any) => 
                                r.T && (fullText += safeDecodeURIComponent(r.T) + ' ')
                            )
                        )
                    );
                    resolve(fullText.trim());
                } catch (error: any) {
                    reject(new Error(`Error extracting text: ${error.message}`));
                }
            });

            pdfParser.parseBuffer(buffer);
        });
    } else if (fileName.endsWith('.docx')) {
        try {
            const result = await mammoth.extractRawText({ buffer });
            return result.value;
        } catch (error: any) {
            throw new Error(`Error extracting DOCX text: ${error.message}`);
        }
    } else if (fileName.endsWith('.txt')) {
        return buffer.toString('utf-8');
    } else {
        throw new Error('Unsupported file type. Please upload a PDF, DOCX, or TXT file.');
    }
}

export async function POST(req: Request) {
    try {
        const file = (await req.formData()).get('file') as File;
        if (!file) {
            return NextResponse.json({ error: 'No file uploaded' }, { status: 400 });
        }

        const documentId = crypto.randomUUID();
        const uploadDate = new Date().toISOString();
        const filePath = `${documentId}.${file.name.split('.').pop() || 'bin'}`;

        // Upload file to Supabase Storage
        const fileBuffer = Buffer.from(await file.arrayBuffer());
        const { error: storageError } = await supabaseStorage.storage
            .from('documents')
            .upload(filePath, fileBuffer, {
                contentType: file.type || 'application/octet-stream',
                cacheControl: '3600',
                upsert: false,
            });

            if (storageError) {
            const msg = storageError.message || 'Unknown storage error';
            if (msg.includes('row-level security') || msg.includes('RLS')) {
                return NextResponse.json({ 
                success: false, 
                error: `Storage RLS error: ${msg}. Ensure SUPABASE_SERVICE_ROLE_KEY is set.` 
                }, { status: 500 });
            }
            return NextResponse.json({ 
                success: false, 
                error: `Failed to store file: ${msg}` 
            }, { status: 500 });
        }

        // Get public URL for the file
        const { data: urlData } = supabaseStorage.storage
        .from('documents')
        .getPublicUrl(filePath);

        // Extract text from file
        const text = await extractTextFromFile(file);
        if (!text || text.trim().length === 0) {
            return NextResponse.json({ 
                error: 'Could not extract text from file' 
            }, { status: 400 });
        }

        // Split text into chunks
        // Chunk size of 800 characters with 100-character overlap ensures
        // we don't lose context at chunk boundaries
        const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: 800,
        chunkOverlap: 100,
        });
        const chunks = await textSplitter.splitText(text);
    
            // Process each chunk: generate embedding and store in database
        for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i];

        // Generate embedding using OpenAI
        // This converts the text chunk into a 1536-dimensional vector
        const emb = await openai.embeddings.create({
            model: 'text-embedding-3-small',
            input: chunk,
        });

        // Store chunk with embedding in database
        const { error } = await supabase.from('documents').insert({
            content: chunk,
            metadata: { 
            source: file.name,
            document_id: documentId,
            file_name: file.name,
            file_type: file.type || file.name.split('.').pop(),
            file_size: file.size,
            upload_date: uploadDate,
            chunk_index: i,
            total_chunks: chunks.length,
            file_path: filePath,
            file_url: urlData.publicUrl,
            },
            embedding: JSON.stringify(emb.data[0].embedding),
        });

        if (error) {
            return NextResponse.json({ 
            success: false, 
            error: error.message 
            }, { status: 500 });
        }
        }

        return NextResponse.json({ 
        success: true, 
        documentId, 
        fileName: file.name, 
        chunks: chunks.length, 
        textLength: text.length, 
        fileUrl: urlData.publicUrl 
        });
    } catch (error: any) {
        return NextResponse.json({ 
        success: false, 
        error: error.message || 'Failed to process file' 
        }, { status: 500 });
    }
}